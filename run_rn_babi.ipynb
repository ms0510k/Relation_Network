{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import wandb\n",
    "from src.models.RN import RelationNetwork\n",
    "from src.nlp_utils import read_babi, vectorize_babi\n",
    "from src.models.LSTM import LSTM\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "from itertools import chain\n",
    "from src.utils import *\n",
    "from task.babi_task.rn.train import train, test, test_separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Argument\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('name', type=str, help='folder in which to store results')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=1, help='epochs to train.')\n",
    "parser.add_argument('--hidden_dims_g', nargs='+', type=int, default=[256, 256, 256], help='layers of relation function g')\n",
    "parser.add_argument('--output_dim_g', type=int, default=256, help='output dimension of relation function g')\n",
    "parser.add_argument('--hidden_dims_f', nargs='+', type=int, default=[256, 512], help='layers of final network f')\n",
    "parser.add_argument('--hidden_dim_lstm', type=int, default=32, help='units of LSTM')\n",
    "parser.add_argument('--lstm_layers', type=int, default=1, help='layers of LSTM')\n",
    "\n",
    "parser.add_argument('--emb_dim', type=int, default=32, help='word embedding dimension')\n",
    "parser.add_argument('--batch_size', type=int, default=3, help='batch size')\n",
    "\n",
    "parser.add_argument('--dropout', action=\"store_true\", help='enable dropout')\n",
    "parser.add_argument('--tanh_act', action=\"store_true\", help='use tanh activation for MLP instead of relu')\n",
    "parser.add_argument('--wave_penc', action=\"store_true\", help='use sin/cos positional encoding instead of one-of-k')\n",
    "\n",
    "\n",
    "# [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "parser.add_argument('--babi_tasks', nargs='+', type=int, default=-1, help='which babi task to train and test. -1 to select all of them.')\n",
    "\n",
    "parser.add_argument('--split_manually', action=\"store_true\", help='Use en-10k folder instead of en-valid-10k folder of babi. Active only with --babi_tasks specified.')\n",
    "parser.add_argument('--only_relevant', action=\"store_true\", help='read only relevant fact from babi dataset. Active only with --split_manually')\n",
    "\n",
    "# optimizer parameters\n",
    "parser.add_argument('--weight_decay', type=float, default=0, help='optimizer hyperparameter')\n",
    "parser.add_argument('--learning_rate', type=float, default=2e-4, help='optimizer hyperparameter')\n",
    "\n",
    "parser.add_argument('--test_on_test', action=\"store_true\", help='final test on test set instead of validation set')\n",
    "parser.add_argument('--test_jointly', action=\"store_true\", help='final test on all tasks')\n",
    "parser.add_argument('--cuda', action=\"store_true\", help='use gpu')\n",
    "parser.add_argument('--load', action=\"store_true\", help=' load saved model')\n",
    "parser.add_argument('--no_save', action=\"store_true\", help='disable model saving')\n",
    "\n",
    "class Namespace:\n",
    "  def __init__(self, **kwargs):\n",
    "    self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(name = 'run_rn_babi', \n",
    "                 epochs = 1, \n",
    "                 hidden_dims_g = [256,256,256], \n",
    "                 output_dim_g = 256, \n",
    "                 hidden_dims_f = [256,512], \n",
    "                 hidden_dim_lstm = 32, \n",
    "                 lstm_layers = 1, \n",
    "                 emb_dim = 32, \n",
    "                 batch_size = 3, \n",
    "                 dropout = False, \n",
    "                 tanh_act = False,\n",
    "                 wave_penc = False,\n",
    "                 babi_tasks = -1,\n",
    "                 split_manually = False,\n",
    "                 only_relevant = False,\n",
    "                 weight_decay = 0,\n",
    "                 learning_rate = 2e-4,\n",
    "                 test_on_test = True,\n",
    "                 test_jointly = False,\n",
    "                 cuda = False,\n",
    "                 load = False,\n",
    "                 no_save = False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/cynics/relation-network-babi\" target=\"_blank\">https://app.wandb.ai/cynics/relation-network-babi</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/cynics/relation-network-babi/runs/6v54dxhv\" target=\"_blank\">https://app.wandb.ai/cynics/relation-network-babi/runs/6v54dxhv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPUs\n"
     ]
    }
   ],
   "source": [
    "if args.batch_size == 1:\n",
    "    print(\"Batch size must be > 1. Setting it to 2.\")\n",
    "    args.batch_size = 2\n",
    "\n",
    "result_folder = get_run_folder(args.name)\n",
    "\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME']=\"run_rn_babi.ipynb\"\n",
    "wandb.init(project=\"relation-network-babi\", name=args.name, config=args, dir=result_folder)\n",
    "\n",
    "mode = 'cpu'\n",
    "if args.cuda:\n",
    "    if torch.cuda.is_available():\n",
    "        print('Using ', torch.cuda.device_count() ,' GPU(s)')\n",
    "        mode = 'cuda'\n",
    "    else:\n",
    "        print(\"WARNING: No GPU found. Using CPUs...\")\n",
    "else:\n",
    "    print('Using 0 GPUs')\n",
    "\n",
    "device = torch.device(mode)\n",
    "\n",
    "cd = os.path.dirname(os.path.abspath(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": [
    "print('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.babi_tasks=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading babi\n",
      "Babi loaded\n"
     ]
    }
   ],
   "source": [
    "if args.babi_tasks == -1: # 20 tasks are already dumped to file\n",
    "    args.babi_tasks = list(range(1,21))\n",
    "    print('Loading babi')\n",
    "    dictionary = load_dict(separately=False) # use always en_valid\n",
    "\n",
    "    train_stories = load_stories(False, 'train')\n",
    "    validation_stories = load_stories(False, 'valid')\n",
    "    if args.test_on_test:\n",
    "        test_stories = load_stories(False, 'test')\n",
    "\n",
    "    print('Babi loaded')\n",
    "\n",
    "else: # single combinations have to be preprocessed from scratch\n",
    "    if not args.split_manually:\n",
    "        path_babi_base = os.path.join(cd, os.path.join(\"babi\", \"en-valid-10k\"))\n",
    "        to_read_test = [files_names_test_en_valid[i-1] for i in args.babi_tasks]\n",
    "        to_read_val = [files_names_val_en_valid[i-1] for i in args.babi_tasks]\n",
    "        to_read_train = [files_names_train_en_valid[i-1] for i in args.babi_tasks]\n",
    "    else:\n",
    "        path_babi_base = os.path.join(cd, os.path.join(\"babi\", \"en-10k\"))\n",
    "        to_read_test = [files_names_test_en[i-1] for i in args.babi_tasks]\n",
    "        to_read_train = [files_names_train_en[i-1] for i in args.babi_tasks]\n",
    "\n",
    "    print(\"Reading babi\")\n",
    "\n",
    "\n",
    "\n",
    "    if args.split_manually: # When reading from en-10k and not from en-valid-10k\n",
    "        stories, dictionary, labels = read_babi(path_babi_base, to_read_train, args.babi_tasks, only_relevant=args.only_relevant)\n",
    "        train_stories, validation_stories = split_train_validation(stories, labels)\n",
    "        train_stories = vectorize_babi(train_stories, dictionary, device)\n",
    "        validation_stories = vectorize_babi(validation_stories, dictionary, device)\n",
    "    else:\n",
    "        train_stories, dictionary, _ = read_babi(path_babi_base, to_read_train, args.babi_tasks, only_relevant=args.only_relevant)\n",
    "        train_stories = vectorize_babi(train_stories, dictionary, device)\n",
    "        validation_stories, _, _ = read_babi(path_babi_base, to_read_val, args.babi_tasks, only_relevant=args.only_relevant)\n",
    "        validation_stories = vectorize_babi(validation_stories, dictionary, device)\n",
    "    if args.test_on_test:\n",
    "        test_stories, _, _ = read_babi(path_babi_base, to_read_test, args.babi_tasks, only_relevant=args.only_relevant)\n",
    "        test_stories = vectorize_babi(test_stories, dictionary, device)\n",
    "\n",
    "\n",
    "# save_dict(dictionary, args.split_manually)\n",
    "# save_stories(train_stories, args.split_manually, 'train')\n",
    "# save_stories(validation_stories, args.split_manually, 'valid')\n",
    "# save_stories(test_stories, args.split_manually, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size:  157\n",
      "Start training\n",
      "Batch  4999 / 53333  - epoch  1 .\n",
      "Batch  9999 / 53333  - epoch  1 .\n",
      "Batch  14999 / 53333  - epoch  1 .\n",
      "Batch  19999 / 53333  - epoch  1 .\n",
      "Batch  24999 / 53333  - epoch  1 .\n",
      "Batch  29999 / 53333  - epoch  1 .\n",
      "Batch  34999 / 53333  - epoch  1 .\n",
      "Batch  39999 / 53333  - epoch  1 .\n",
      "Batch  44999 / 53333  - epoch  1 .\n",
      "Batch  49999 / 53333  - epoch  1 .\n",
      "Test batch:  999 / 13333\n",
      "Test batch:  1999 / 13333\n",
      "Test batch:  2999 / 13333\n",
      "Test batch:  3999 / 13333\n",
      "Test batch:  4999 / 13333\n",
      "Test batch:  5999 / 13333\n",
      "Test batch:  6999 / 13333\n",
      "Test batch:  7999 / 13333\n",
      "Test batch:  8999 / 13333\n",
      "Test batch:  9999 / 13333\n",
      "Test batch:  10999 / 13333\n",
      "Test batch:  11999 / 13333\n",
      "Test batch:  12999 / 13333\n",
      "Train loss:  1.3425924719704248 . Validation loss:  1.0556051932053858\n",
      "Train accuracy:  0.38570241064007266 . Validation accuracy:  0.4980874521863056\n",
      "\n",
      "End training!\n",
      "Testing separately...\n",
      "Batch within test:  0 / 6666\n",
      "Batch within test:  1000 / 6666\n",
      "Batch within test:  2000 / 6666\n",
      "Batch within test:  3000 / 6666\n",
      "Batch within test:  4000 / 6666\n",
      "Batch within test:  5000 / 6666\n",
      "Batch within test:  6000 / 6666\n",
      "Test accuracy:  {1: 0.434, 2: 0.309, 3: 0.281, 4: 0.453, 5: 0.364, 6: 0.607, 7: 0.602, 8: 0.368, 9: 0.65, 10: 0.463, 11: 0.597, 12: 0.363, 13: 0.65, 14: 0.325, 15: 0.414, 16: 0.422, 17: 0.5, 18: 0.912, 19: 0.256, 20: 0.8547094188376754}\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    # if m.dim() > 1:\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "dict_size = len(dictionary)\n",
    "print(\"Dictionary size: \", dict_size)\n",
    "\n",
    "lstm = LSTM(args.hidden_dim_lstm, args.batch_size, dict_size, args.emb_dim, args.lstm_layers, device, wave_penc=args.wave_penc, dropout=args.dropout).to(device)\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "rn = RelationNetwork(args.hidden_dim_lstm, args.hidden_dims_g, args.output_dim_g, args.hidden_dims_f, dict_size, args.dropout, args.tanh_act, args.batch_size, args.wave_penc, device).to(device)\n",
    "rn.apply(init_weights)\n",
    "\n",
    "wandb.watch(rn)\n",
    "wandb.watch(lstm)\n",
    "\n",
    "if args.load:\n",
    "    load_models([(lstm, names_models[0]), (rn, names_models[1])], result_folder, saving_path_rn)\n",
    "\n",
    "optimizer = torch.optim.Adam(chain(lstm.parameters(), rn.parameters()), args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "if args.epochs > 0:\n",
    "    print(\"Start training\")\n",
    "    avg_train_losses, avg_train_accuracies, val_losses, val_accuracies = train(train_stories, validation_stories, args.epochs, lstm, rn, criterion, optimizer, args.no_save, device, result_folder, args.batch_size)\n",
    "    print(\"End training!\")\n",
    "\n",
    "if not args.test_on_test:\n",
    "    test_stories = validation_stories\n",
    "\n",
    "if args.test_jointly:\n",
    "    print(\"Testing jointly...\")\n",
    "    avg_test_loss, avg_test_accuracy = test(test_stories, lstm, rn, criterion, device, args.batch_size)\n",
    "\n",
    "    print(\"Test accuracy: \", avg_test_accuracy)\n",
    "    print(\"Test loss: \", avg_test_loss)\n",
    "else:\n",
    "    print(\"Testing separately...\")\n",
    "    avg_test_accuracy = test_separately(test_stories, lstm, rn, device, args.batch_size)\n",
    "    avg_test_loss = None\n",
    "    print(\"Test accuracy: \", avg_test_accuracy)\n",
    "\n",
    "    write_test(result_folder, losses=avg_test_loss, accs=avg_test_accuracy)\n",
    "\n",
    "if args.epochs > 0:\n",
    "    plot_results(result_folder, avg_train_losses, val_losses, avg_train_accuracies, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rn] *",
   "language": "python",
   "name": "conda-env-rn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
